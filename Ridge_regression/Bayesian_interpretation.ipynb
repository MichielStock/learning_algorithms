{
 "metadata": {
  "name": "",
  "signature": "sha256:844420cdbe3b2759c7f01fd86a22bcbb319b56b845c079730bbe67563f1f6150"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sympy as sp\n",
      "from sympy.utilities.lambdify import lambdify\n",
      "sp.init_printing()\n",
      "# commands starting with % are IPython commands"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Kronecker RLS: going Bayesian\n",
      "\n",
      "In this small draft we will turn the Kroncker RLS model into a Gaussian Process.\n",
      "\n",
      "Let $Y$ be the $N\\times M$ matrix containing the observations, associated with two kernel matrices: $K$ and $G$, for which we define the pairwise kernel matrix:\n",
      "\n",
      "$$\n",
      "\\Gamma = G \\otimes K.\n",
      "$$\n",
      "\n",
      "We will make the differnce between $Y$, the observed labels, which is a noise representation of the outcome of our function $f$, collected in the matrix $F$. We define a prior on this funnction:\n",
      "\n",
      "$$\n",
      "\\mathcal{P}(F) = \\mathcal{N}(Vec (F) | \\mathbf{0}, \\sigma_w^2 \\Gamma),\n",
      "$$\n",
      "\n",
      "and likelihood:\n",
      "\n",
      "$$\n",
      "\\mathcal{P}(Y|F) = \\mathcal{N}(Vec (Y) | Vec (F), \\sigma_v^2 I).\n",
      "$$\n",
      "\n",
      "Using Bayes' theorem we obtain:\n",
      "\n",
      "$$\n",
      "\\mathcal{P}(Y) = \\mathcal{N}(Vec (Y) | \\mathbf{0}, \\sigma_w^2 \\Gamma + \\sigma_v^2 I).\n",
      "$$\n",
      "\n",
      "For ease of the notation\n",
      "\n",
      "$$\n",
      "A = \\sigma_w^2 \\Gamma + \\sigma_v^2 I,\n",
      "$$\n",
      "\n",
      "which can easily be manipulated using the eigenvalue decomposition of the kernel matrices:\n",
      "\n",
      "$$\n",
      "A = (V \\otimes U)(\\sigma_w^2 (\\Lambda \\otimes \\Sigma) + \\sigma_v^2 I)(V^\\intercal \\otimes U^\\intercal),\n",
      "$$\n",
      "so inverse and determinant is trivial.\n",
      "\n",
      "We can calculate the logarithm of the evidence, and maximise this to find the hyperparameters:\n",
      "\n",
      "$$\n",
      "\\log(\\mathcal{P}(Y)) = -\\frac{NM}{2} -\\frac{1}{2}\\log(|A|) -\\frac{1}{2} Vec(Y)^T A^{-1} Vec(Y).\n",
      "$$\n",
      "\n",
      "We can (of course) make predictions, for which the mean of the posterior distribution is given by:\n",
      "\n",
      "$$\n",
      "(\\mathbf{g} \\otimes \\mathbf{k})^\\intercal A^{-1} Vec (Y),\n",
      "$$\n",
      "\n",
      "and the variance (covariance) matrix can also easily be determined."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}