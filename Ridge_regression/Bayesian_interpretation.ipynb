{
 "metadata": {
  "name": "",
  "signature": "sha256:20af8a129803488178e50dc7ff0daa1edffa214751df597cc2a135b57e33b165"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sympy as sp\n",
      "from sympy.utilities.lambdify import lambdify\n",
      "sp.init_printing()\n",
      "# commands starting with % are IPython commands"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Kronecker RLS: going Bayesian\n",
      "\n",
      "**Michiel Stock**\n",
      "\n",
      "In this small draft we will turn the Kroncker RLS model into a Gaussian Process.\n",
      "\n",
      "Let $Y$ be the $N\\times M$ matrix containing the observations, associated with two kernel matrices: $K$ and $G$, for which we define the pairwise kernel matrix:\n",
      "\n",
      "$$\n",
      "\\Gamma = G \\otimes K.\n",
      "$$\n",
      "\n",
      "We will make the differnce between $Y$, the observed labels, which is a noise representation of the outcome of our function $f$, collected in the matrix $F$. We define a prior on this funnction:\n",
      "\n",
      "$$\n",
      "\\mathcal{P}(F) = \\mathcal{N}(Vec (F) | \\mathbf{0}, \\sigma_w^2 \\Gamma),\n",
      "$$\n",
      "\n",
      "and likelihood:\n",
      "\n",
      "$$\n",
      "\\mathcal{P}(Y|F) = \\mathcal{N}(Vec (Y) | Vec (F), \\sigma_v^2 I).\n",
      "$$\n",
      "\n",
      "Using Bayes' theorem we obtain:\n",
      "\n",
      "$$\n",
      "\\mathcal{P}(Y) = \\mathcal{N}(Vec (Y) | \\mathbf{0}, \\sigma_w^2 \\Gamma + \\sigma_v^2 I).\n",
      "$$\n",
      "\n",
      "For ease of the notation\n",
      "\n",
      "$$\n",
      "A = \\sigma_w^2 \\Gamma + \\sigma_v^2 I,\n",
      "$$\n",
      "\n",
      "which can easily be manipulated using the eigenvalue decomposition of the kernel matrices:\n",
      "\n",
      "$$\n",
      "A = (V \\otimes U)(\\sigma_w^2 (\\Lambda \\otimes \\Sigma) + \\sigma_v^2 I)(V^\\intercal \\otimes U^\\intercal),\n",
      "$$\n",
      "so inverse and determinant is trivial.\n",
      "\n",
      "We can calculate the logarithm of the evidence, and maximise this to find the hyperparameters:\n",
      "\n",
      "$$\n",
      "\\log(\\mathcal{P}(Y)) = -\\frac{NM}{2} -\\frac{1}{2}\\log(|A|) -\\frac{1}{2} Vec(Y)^T A^{-1} Vec(Y).\n",
      "$$\n",
      "\n",
      "We can (of course) make predictions, for which the mean of the posterior distribution is given by:\n",
      "\n",
      "$$\n",
      "(\\mathbf{g} \\otimes \\mathbf{k})^\\intercal A^{-1} Vec (Y),\n",
      "$$\n",
      "\n",
      "and the variance (covariance) matrix can also easily be determined."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Two-step RLS\n",
      "There seems not to be an elegant way to encode the two-step RLS as a GP. Because in the second step, both the features (output first model) and the labels are stochastic, they cannot be discribed with a MVN.\n",
      "\n",
      "We recommend using the following kernel for 2SRLS:\n",
      "\n",
      "$$\n",
      "\\Gamma = (G + \\sigma_v I)\\otimes(K + \\sigma_u I).\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Discussion\n",
      "This framework is interesting for two important reasons:\n",
      "\n",
      "- because the everything is probabilistic, we have the means of designing experiments: e.g. suppose we want to find a bioactive compound from a large database by a laboratory test. Using this framework, the joint posterior probabillity of a given activity is given by a multivariate normal distribution. The problem could be defined as finding a subset of compounds to test that contains at least one powerful compound (i.e. a combinatorial optimisation problem).\n",
      "- The posterior likelihood of the data $\\mathcal{P}(\\mathcal{D}|\\theta)$ allows us to use gradient-based methods to find the hyperparameters, without resorting to validation sets. From a small experiment I found that this works, but we do not end up with a model as good as using cross-validation. Still, when having maning hyperparameters, this approach seems viable. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}